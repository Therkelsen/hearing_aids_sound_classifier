{"cells":[{"cell_type":"markdown","metadata":{"id":"eCXIDdkhXPGp"},"source":["This first block of code simply imports all of our dependencies and sets some environment settings for our code."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSvA7tIOEO_D"},"outputs":[],"source":["!pip install livelossplot\n","!pip install -q -U keras-tuner\n","!pip install visualkeras\n","\n","import keras_tuner as kt\n","import librosa\n","import math\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import pandas as pd\n","import pathlib\n","import random\n","import scipy.io as sio\n","import seaborn as sns\n","import shutil\n","import sys\n","import tensorflow as tf\n","import time\n","import visualkeras\n","\n","from collections import defaultdict\n","from google.colab import drive\n","from keras import regularizers\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","from keras.preprocessing.image import ImageDataGenerator, NumpyArrayIterator\n","from keras.utils import to_categorical\n","from keras.utils.vis_utils import plot_model\n","from livelossplot import PlotLossesKeras\n","from os import listdir\n","from os.path import dirname, join as pjoin\n","from scipy import signal\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n","from tensorflow.keras.utils import to_categorical, image_dataset_from_directory\n","from tensorflow.keras import optimizers, regularizers\n","from tensorflow.keras.models import load_model, Model, Sequential\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, LeakyReLU\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.python.client import device_lib\n","\n","%matplotlib inline\n","mpl.rcParams['figure.dpi'] = 200\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  print('GPU device not found\\nProceeding without one.')\n","else:\n","  print('Found GPU at: {}'.format(device_name))\n","\n","# This is a bit of magic to make matplotlib figures appear inline in the\n","# notebook rather than in a new window.\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (2.0, 2.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","# Some more magic so that the notebook will reload external python modules;\n","# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n","%load_ext autoreload\n","%autoreload 2\n","\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"PSb-xajoXb2K"},"source":["**Load and Plot dataset**\n","\n","This block of code loads the dataset, and then plots a sample of the training data as both Spectogram and Waveform."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2M35eNlUF_z2"},"outputs":[],"source":["# Set default data path\n","# data_dir = 'data/'\n","data_dir = '/content/drive/My Drive/Oticon/hearing_aids_sound_classifier/data/'\n","\n","# Load the training data\n","training_data = np.load(data_dir + 'npy/training.npy')\n","# Load the training labels\n","training_labels = np.load(data_dir + 'npy/training_labels.npy')\n","\n","# Find NaN samples in training_data\n","nan_samples = np.isnan(training_data)\n","\n","# Find the indices of NaN samples\n","nan_indices = np.unique(np.where(nan_samples)[0])\n","\n","# Remove NaN samples from training_data and training_labels\n","training_data = np.delete(training_data, nan_indices, axis=0)\n","training_labels = np.delete(training_labels, nan_indices, axis=0)\n","\n","# Test shape of data\n","print(\"Training data shape: \", training_data.shape)\n","print('Training data datatype: ', training_data.dtype)\n","\n","# Set up subplots\n","fig, axs = plt.subplots(ncols=2, figsize=(12, 5))\n","\n","# Select a random sound sample\n","sample = training_data[random.randint(0, len(training_data) - 1)]\n","\n","\n","# Create a time array for the sample\n","t = np.linspace(0, 2, len(sample[0]))\n","\n","# Create a frequency array for the sample\n","f = np.linspace(0, 11025 , len(sample), endpoint=False)\n","\n","# Plot the spectrogram of the audio sample\n","axs[0].imshow(sample.T, origin='lower', aspect='auto', cmap='inferno', extent=[0, 2, 0, 11025])\n","axs[0].set_xlabel('Time (seconds)')\n","axs[0].set_ylabel('Frequency (Hz)')\n","axs[0].set_title('Spectrogram of Audio Sample')\n","fig.colorbar(axs[0].images[0], ax=axs[0])\n","\n","# Create a time array for the sample\n","t = np.linspace(0, 2, len(sample))\n","\n","# Plot the audio waveform\n","axs[1].plot(t, sample)\n","axs[1].set_xlabel('Time (seconds)')\n","axs[1].set_ylabel('Amplitude (dB)')\n","axs[1].set_title('Audio Waveform')\n","\n","# Adjust spacing between subplots\n","fig.tight_layout()\n","\n","# Display the plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0Z4GrjYbX2jP"},"source":["**Data preprocessing**\n","\n","This block of code normalises the dataset, as well as splits the data into a 80/10/10 training/validation/test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e_zwZ2c4TRIA"},"outputs":[],"source":["# Calculate the mean and standard deviation of the training data\n","training_mean = np.mean(training_data, axis=0)\n","training_std = np.std(training_data, axis=0)\n","\n","# Normalize the training data\n","training_data_norm = (training_data - training_mean) / training_std\n","\n","# # THIS IS NOT USED, AS IT DOES NOT IMPROVE ROBUSTNESS NOR ACCURACY\n","# # calculate z-score of each sample in the dataset\n","# z_scores = np.abs((training_data_norm - np.mean(training_data_norm)) / np.std(training_data_norm))\n","\n","# # threshold for outlier detection\n","# outlier_threshold = 3.50\n","# # find samples with z-score higher than the threshold in any of the features\n","# outliers = np.where(np.max(z_scores, axis=1) > outlier_threshold)[0]\n","\n","# # remove outliers from training data and labels\n","# training_data_clean = np.delete(training_data_norm, outliers, axis=0)\n","# training_labels_clean = np.delete(training_labels, outliers)\n","\n","# # print number of removed samples\n","# num_removed = len(outliers)\n","# print(f\"Number of removed samples: {num_removed}\")\n","\n","# Reshape the data\n","training_data_norm = training_data_norm.reshape(-1, 96, 32, 1)\n","\n","# Split your data into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(training_data_norm, training_labels, test_size=0.2, random_state=int(time.time()))\n","\n","# Split the validation set further into validation and test sets\n","X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=int(time.time()))\n","\n","# Setting up the datagenerators\n","# Define your ImageDataGenerator for data augmentation\n","train_datagen = ImageDataGenerator(\n","    rotation_range=10,\n","    zoom_range=0.2,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    fill_mode=\"nearest\"\n",")\n","\n","# One-hot encode the labels\n","y_train = tf.keras.utils.to_categorical(y_train, num_classes=5, dtype='float32')\n","y_val = tf.keras.utils.to_categorical(y_val, num_classes=5, dtype='float32')\n","y_test = tf.keras.utils.to_categorical(y_test, num_classes=5, dtype='float32')\n","\n","# Verify proper data preprocessing\n","print('Raw data min & max:', training_data.min(), training_data.max())\n","print('Normalized data min & max:', training_data_norm.min(), training_data_norm.max())\n","print('Processed data min & max:', X_train.min(), X_train.max())\n","\n","# Test shapes of the sets\n","print('Training data shape: ', X_train.shape)\n","print('Training labels shape: ', y_train.shape)\n","print('Validation data shape: ', X_val.shape)\n","print('Validation labels shape: ', y_val.shape)\n","print('Test data shape: ', X_test.shape)\n","print('Test labels shape: ', y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"rtiYs9kCYCO9"},"source":["**Setup of NN**\n","\n","Considering the size of the dataset and the nature of the task (sound classification), a Convolutional Neural Network (CNN) would be a good choice. CNNs are effective in processing and extracting features from images and sound, and they are computationally efficient due to their shared weight architecture.\n","\n","This uses Keras' automatic hyperparameter tuner to find the best possible combination for our case."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K6ZclPEtYM_S"},"outputs":[],"source":["# The types of sounds that exist in the dataset\n","num_classes = 5\n","\n","def model_builder(hp):\n","  model = tf.keras.Sequential()\n","  hp_layer_1 = hp.Int('layer_1', min_value = 8, max_value=128, step=8)\n","  hp_layer_2 = hp.Int('layer_2', min_value = 8, max_value=128, step=16)\n","  hp_layer_3 = hp.Int('layer_3', min_value = 8, max_value=128, step=32)\n","  hp_layer_4 = hp.Int('layer_4', min_value = 8, max_value=128, step=32)\n","  hp_dropout_1 = hp.Float('dropout_1', min_value=0.0, max_value=0.8, step=0.05)\n","  hp_dropout_2 = hp.Float('dropout_2', min_value=0.0, max_value=0.8, step=0.05)\n","  hp_alpha = hp.Float('alpha_1', min_value=0.001, max_value=0.3, step=0.001)\n","  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n","    \n","  model.add(tf.keras.layers.Conv2D(filters=hp_layer_1, kernel_size=(5,3), activation=LeakyReLU(alpha=hp_alpha), input_shape=(96, 32, 1)))\n","  model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n","  model.add(tf.keras.layers.Dropout(hp_dropout_1))\n","  model.add(tf.keras.layers.Conv2D(filters=hp_layer_2, kernel_size=(3,3), activation=LeakyReLU(alpha=hp_alpha)))\n","  model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n","  model.add(tf.keras.layers.Conv2D(filters=hp_layer_3, kernel_size=(3,3), activation=LeakyReLU(alpha=hp_alpha)))\n","  model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n","  model.add(tf.keras.layers.Flatten())\n","  model.add(tf.keras.layers.Dense(units=hp_layer_4, activation=LeakyReLU(alpha=hp_alpha)))\n","  model.add(tf.keras.layers.Dropout(hp_dropout_2))\n","  model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n","\n","  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=['accuracy'])\n","\n","  return model\n","\n","tuner = kt.Hyperband(model_builder,\n","                     objective='val_accuracy',\n","                     max_epochs=25,\n","                     directory='dir',\n","                     project_name='x')\n","\n","early_stop = EarlyStopping(patience=5, verbose=1)\n","reduce_lr = ReduceLROnPlateau(factor=0.1, patience=5, verbose=1)"]},{"cell_type":"markdown","metadata":{"id":"GD6DFjp5ZI7W"},"source":["**Hyperparameter tuning**\n","\n","Next, the tuner tries a TON of combinations and saves the best one."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"72xNCxl0ZQuX"},"outputs":[],"source":["# How many epochs to train for\n","num_epochs = 50\n","\n","# Batch size for training\n","batch_size = 32\n","\n","verbosity = 2\n","\n","tuner.search(X_train, y_train, epochs=num_epochs, validation_data=(X_val, y_val), callbacks=[early_stop, reduce_lr])"]},{"cell_type":"markdown","metadata":{"id":"WfjdwQQTa53e"},"source":["**Model training**\n","\n","Here, the model with the best hyperparameters is made and then trained on the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MUzaI_dLZ8mB"},"outputs":[],"source":["best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n","\n","model = tuner.hypermodel.build(best_hps)\n","\n","# Get a summary of the model\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"RxKbhlCVSsVD"},"source":["If one wants to manually set up the CNN, that can be done here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"meSiVXweSWBV"},"outputs":[],"source":["# The types of sounds that exist in the dataset\n","num_classes = 5\n","\n","# Define the CNN model\n","model = Sequential([\n","    Conv2D(32, (5, 3), activation=LeakyReLU(alpha=0.035), input_shape=(96, 32, 1)),\n","    BatchNormalization(),\n","    MaxPooling2D((2, 2)),\n","    Dropout(0.5),\n","    Conv2D(48, (3, 3), activation=LeakyReLU(alpha=0.035)),\n","    BatchNormalization(),\n","    MaxPooling2D((2, 2)),\n","    Dropout(0.25),\n","    Conv2D(128, (3, 3), activation=LeakyReLU(alpha=0.035)),\n","    BatchNormalization(),\n","    MaxPooling2D((2, 2)),\n","    Flatten(),\n","    Dense(96, activation=LeakyReLU(alpha=0.1)),\n","    BatchNormalization(),\n","    Dense(num_classes, activation='softmax')\n","])\n","\n","# Compile the model with categorical crossentropy loss and Adam optimizer\n","model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Ad0am(learning_rate=1e-2), metrics=['Accuracy'])\n","# Get a summary of the model\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gh6KceLgfzGc"},"outputs":[],"source":["# Define the callbacks\n","early_stop = EarlyStopping(patience=10, verbose=1)\n","# model_checkpoint = ModelCheckpoint('models/best_model.h5', save_best_only=True, verbose=1)\n","model_checkpoint = ModelCheckpoint('/content/drive/My Drive/Oticon/hearing_aids_sound_classifier/models/my_model.h5', save_best_only=True, verbose=1)\n","reduce_lr = ReduceLROnPlateau(factor=0.9, patience=5, verbose=1)\n","\n","# How many epochs to train for\n","num_epochs = 500\n","# Batch size for training\n","batch_size = 32\n","# Verbosity of training\n","verbosity = 1\n","# Train the model on fixed dataset with callbacks\n","start = time.process_time()\n","history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size,\n","          validation_data=(X_val, y_val), verbose=verbosity,\n","          callbacks=[early_stop, model_checkpoint, reduce_lr])\n","\n","# # fits the model on batches with real-time data augmentation\n","# train_datagen.fit(X_train)\n","# history = model.fit(train_datagen.flow(X_train, y_train, batch_size=batch_size),\n","#          validation_data=(X_val, y_val),\n","#          steps_per_epoch=len(X_train) / batch_size, epochs=num_epochs,\n","#          callbacks=[early_stop, model_checkpoint, reduce_lr])\n","print('Training the model took ', (time.process_time() - start)/60, 'minutes')"]},{"cell_type":"markdown","metadata":{"id":"-y4tmwN7oAeL"},"source":["Load the best model so far and evaluate it on the validation set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8V-6QAofbaF"},"outputs":[],"source":["# Load the desired model from the models folder\n","model = tf.keras.models.load_model('/content/drive/My Drive/Oticon/hearing_aids_sound_classifier/models/317k_LReLU_97-percent.h5')\n","\n","# Get a summary of the model\n","model.summary(expand_nested=True)\n","\n","color_map = defaultdict(dict)\n","color_map[Dense]['fill'] = 'orange'\n","\n","visualkeras.layered_view(model, legend=True, color_map=color_map).show()\n","\n","plot_model(model, to_file='/content/drive/My Drive/Oticon/hearing_aids_sound_classifier/models/visualisations/model_plot.png', show_shapes=True, show_layer_names=False)\n","\n","# Test the model on the validation set (since we have no labels for the test set)\n","model.evaluate(X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ttSPhkGyjwW9"},"outputs":[],"source":["# Save model\n","model.save('/content/drive/My Drive/Oticon/hearing_aids_sound_classifier/models/my_model.h5')"]},{"cell_type":"markdown","source":["**Confusion Matrix**\n"],"metadata":{"id":"Q4w2nB0Gdzql"}},{"cell_type":"code","source":["# Assume that you have a trained model called 'model'\n","y_pred = model.predict(X_test)  # Make predictions on X_test\n","\n","# convert the predicted probabilities to predicted class labels\n","y_pred_labels = np.argmax(y_pred, axis=1)\n","y_true = np.argmax(y_test, axis=1)\n","\n","# Assume that the true labels for X_test are stored in 'y_true'\n","# confusion_mat = confusion_matrix(y_true, y_pred)\n","# print(confusion_mat)\n","classes = [\"Other\", \"Music\", \"Voice\", \"Engine\", \"Alarm\"]\n","\n","cm = confusion_matrix(y_true, y_pred_labels)\n","print(\"Confusion matrix\")\n","print(cm)\n","print(\"\\nConfusion matrix (normalised)\")\n","cm_norm = cm / cm.astype(float).sum(axis=1)\n","# cm_norm = [['{:.4f}'.format(item) for item in sublist] for sublist in (cm / cm.astype(float).sum(axis=1))]\n","print(cm_norm)"],"metadata":{"id":"PFK091sYoOOM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Making histogram over class frequency**"],"metadata":{"id":"K1pqXqWrCmEG"}},{"cell_type":"code","source":["# Calculating number of samples for every class\n","# Iterating all classes' indexes in 'y_true' array\n","# Using Numpy function 'unique'\n","# Returning sorted unique elements and their frequencies\n","unique, counts = np.unique(y_true, return_counts=True)\n","\n","# Setting default size of the plot\n","plt.rcParams['figure.figsize'] = (10.0, 7.0)\n","\n","# Plotting histogram of 5 classes with their number of samples\n","# Defining a figure object \n","figure = plt.figure()\n","\n","plt.bar(unique, counts, align='center', alpha=0.6)\n","\n","plt.ylabel(\"Class frequency\", fontsize=15)\n","\n","plt.xticks(np.arange(5), classes)\n","\n","plt.title(\"Class frequency Histogram\", fontsize=20)\n","\n","\n","# Saving the plot\n","figure.savefig('histogram.png', transparent=True, dpi=500)\n","\n","# Showing the plot\n","plt.show()"],"metadata":{"id":"jnzacpj7p6EG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Displaying Confusion Matrix**"],"metadata":{"id":"hXErdIYTuTOV"}},{"cell_type":"code","source":["# Setting default size of the plot\n","# Setting default fontsize used in the plot\n","plt.rcParams['figure.figsize'] = (10.0, 9.0)\n","plt.rcParams['font.size'] = 20\n","\n","# Implementing visualization of Confusion Matrix\n","display_cm = ConfusionMatrixDisplay(cm_norm, display_labels=classes)\n","# Normalised cm\n","# display_cm = ConfusionMatrixDisplay(cm_norm, display_labels=classes)\n","\n","# Plotting Confusion Matrix\n","# Setting colour map to be used\n","display_cm.plot(cmap='OrRd', xticks_rotation=25)\n","# Other possible options for colour map are:\n","# 'autumn_r', 'Blues', 'cool', 'Greens', 'Greys', 'PuRd', 'copper_r'\n","\n","# Setting fontsize for xticks and yticks\n","plt.xticks(fontsize=15)\n","plt.yticks(fontsize=15)\n","\n","# Giving name to the plot\n","plt.title('Confusion Matrix', fontsize=24)\n","\n","# Saving plot\n","plt.savefig('confusion_matrix.png', transparent=True, dpi=500)\n","\n","# Showing the plot\n","plt.show()"],"metadata":{"id":"wuGpvFf3uZiP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the desired model from the models folder\n","model = tf.keras.models.load_model('/content/drive/My Drive/Oticon/hearing_aids_sound_classifier/models/317k_LReLU_97-percent.h5')\n","\n","# Load the unlabeled test data\n","unlabeled_test_data = np.load(data_dir + 'npy/test.npy')\n","\n","unlabeled_test_data = unlabeled_test_data.reshape(-1, 96, 32, 1)\n","\n","# Get a summary of the model\n","model.summary(expand_nested=True)\n","\n","# Predict the test set\n","preds_labels = model.predict(unlabeled_test_data)\n","\n","# Get highest probable prediction for each sample\n","preds_labels = np.argmax(preds_labels, axis=1)\n","\n","print('predicted labels array size: ', preds_labels.size)\n","print('predicted labels array shape: ', preds_labels.shape)\n","print('predicted labels array dtype: ', preds_labels.dtype)\n","print(preds_labels)\n","\n","np.savetxt(data_dir + \"predictions/predictions.txt\", preds_labels, fmt='%i', header='', comments='')\n","\n","with open(data_dir + \"predictions/predictions.txt\") as f:\n","    row_count = sum(1 for line in f)\n","print(row_count)"],"metadata":{"id":"0d6dRiTiQhJi"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}